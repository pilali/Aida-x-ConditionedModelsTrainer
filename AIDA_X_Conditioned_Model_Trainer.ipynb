{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH1EG9tynf_l"
      },
      "source": [
        "# Introduction ‚ú®\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_h4O_ErtSTY"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1TuesF83uT3BoShpMgIW5NN2itlNIwGX-)\n",
        "![](https://drive.google.com/uc?export=view&id=11FoXiDS0XcQG5R9zUh6luvjfJxxQ3vYT)\n",
        "![](https://drive.google.com/uc?export=view&id=145LaNxAZsxPzXoOuFOxJxak_1J90fs5l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QA4dyyWzDPh"
      },
      "source": [
        "This notebook is a first draft for training conditioned models for the [AIDA-X](https://github.com/AidaDSP/aida-x) plugin. If run inside of Colab, it will automatically use a free Google Cloud GPU.\n",
        "\n",
        "*** YOU WILL LIKELY NEED TO BUY UNITS TO BE ABLE TO COMPLETE THE TRAINING. ***\n",
        "\n",
        "At the end, you'll have a custom-trained model that you can download and play directly on AIDA-X plugin.\\\n",
        "[DEMO VIDEO]() üîäüîäüîä\n",
        "\n",
        "---\n",
        "This notebook relies on the worls by the [MOD Audio](https://mod.audio) and the [AIDA DSP](https://aidadsp.github.io) teams.\\\n",
        "Some of the code and workflow presented here is inspired by the [NAM](https://github.com/sdatkinson/neural-amp-modeler) training [colab](https://colab.research.google.com/github/sdatkinson/neural-amp-modeler/blob/main/bin/train/easy_colab.ipynb?authuser=1#scrollTo=5CQleTk7GJV8) notebook.\n",
        "\n",
        "---  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96GDn8reahv3"
      },
      "source": [
        "## **Instructions** ([step-by-step video](https://www.youtube.com/watch?v=htpK0QLzeKA))\n",
        "\n",
        "The goal is to capture your amp or pedal and the behaviour of two features, for example the Gain knob and the Tone knkob. To get there you will have to reamp your input.wav signal several times to provide the needed files listed below:\n",
        "\n",
        "* target_gain0.0_tone1.0.wav\n",
        "* target_gain0.1_tone1.0.wav\n",
        "* target_gain0.2_tone1.0.wav\n",
        "* target_gain0.3_tone1.0.wav\n",
        "* target_gain0.4_tone1.0.wav\n",
        "* target_gain0.5_tone0.0.wav\n",
        "* target_gain0.5_tone0.1.wav\n",
        "* target_gain0.5_tone0.2.wav\n",
        "* target_gain0.5_tone0.3.wav\n",
        "* target_gain0.5_tone0.4.wav\n",
        "* target_gain0.5_tone0.5.wav\n",
        "* target_gain0.5_tone0.6.wav\n",
        "* target_gain0.5_tone0.7.wav\n",
        "* target_gain0.5_tone0.8.wav\n",
        "* target_gain0.5_tone0.9.wav\n",
        "* target_gain0.5_tone1.0.wav\n",
        "* target_gain0.6_tone1.0.wav\n",
        "* target_gain0.7_tone1.0.wav\n",
        "* target_gain0.8_tone1.0.wav\n",
        "* target_gain0.9_tone1.0.wav\n",
        "* target_gain1.0_tone1.0.wav\n",
        "\n",
        "Correct results can be achieved with less files. At this point you will need to edit the code in the cells if you want to use less files.\n",
        "\n",
        "Whenever you see `<- RUN CELL (‚ñ∫)`, you need to press the (‚ñ∫) next to it, to run the code that will fulfill that step.  \n",
        "\n",
        "> The steps in this notebook are pretty straightforward:\n",
        "0.   Deps üëæ\n",
        "1.   Set-up üëæ\n",
        "2.   Data üìë\n",
        "3.   Model Training üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
        "4.   Model Evaluation üìà (optional)\n",
        "5.   Model Export ‚úÖ\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87p4ClYWZSVz"
      },
      "source": [
        "# 0. Deps üëæ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n",
        "# Check PyTorch and CUDA versions\n",
        "import torch\n",
        "import re\n",
        "\n",
        "pytorch_version = torch.__version__\n",
        "cuda_version = torch.version.cuda\n",
        "\n",
        "required_pytorch_version = \"2.3.1\"\n",
        "required_cuda_version = \"12.1\"\n",
        "\n",
        "def version_higher(version1, version2):\n",
        "  def extract_numeric_version(version):\n",
        "    return tuple(map(int, re.findall(r'\\d+', version)))\n",
        "  return extract_numeric_version(version1) > extract_numeric_version(version2)\n",
        "\n",
        "if version_higher(pytorch_version, required_pytorch_version) or version_higher(cuda_version, required_cuda_version):\n",
        "  print(f\"WARNING: Your environment has PyTorch {pytorch_version} and CUDA {cuda_version}. This environment is not supported.\")\n",
        "  print(\"Proceeding to install required dependencies...\")\n",
        "  !pip3 uninstall --disable-pip-version-check -y torch torchvision torchaudio\n",
        "  !pip3 install --disable-pip-version-check --no-cache-dir \\\n",
        "    torch==2.3.1+cu121 \\\n",
        "    torchvision==0.18.1+cu121 \\\n",
        "    torchaudio==2.3.1+cu121 \\\n",
        "    -f https://download.pytorch.org/whl/torch_stable.html\n",
        "  print(\"PyTorch and CUDA versions have been set to the required versions. Please restart the runtime.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Set-up üëæ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HQzdKDSc0NId"
      },
      "outputs": [],
      "source": [
        "#@markdown `<- RUN CELL (‚ñ∫)`\n",
        "\n",
        "#@markdown This will check for GPU availability, prepare the code for you, and mount your drive.\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import IPython\n",
        "from time import sleep\n",
        "import librosa\n",
        "\n",
        "print(\"---\")\n",
        "if 'step' in locals():\n",
        "  print(\"Ready! you can now move to step 1: DATA\")\n",
        "else:\n",
        "\n",
        "  print(\"Checking GPU availability...\", end=\" \")\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU available! \")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU unavailable, using CPU instead.\")\n",
        "    print(\"RECOMMENDED: You can enable GPU through \\\"Runtime\\\" -> \\\"Change runtime type\\\" -> \\\"Hardware accelerator:\\\" GPU -> Save\")\n",
        "\n",
        "  if any(key.startswith(\"COLAB_\") for key in os.environ):\n",
        "    if not os.path.exists(\"/content/Aida-x-ConditionedModelsTrainer\"):\n",
        "      print(\"Getting the code...\")\n",
        "      !git clone https://github.com/pilali/Aida-x-ConditionedModelsTrainer.git &>> /content/log.txt\n",
        "      assert os.path.exists(\"/content/Aida-x-ConditionedModelsTrainer\"), f\"Error getting the code!\"\n",
        "\n",
        "      os.chdir('/content/Aida-x-ConditionedModelsTrainer')\n",
        "      !git checkout badcat &>> /content/log.txt\n",
        "\n",
        "#      print(\"Checking for code updates...\")\n",
        "#      !git submodule update --init --recursive &>> /content/log.txt\n",
        "\n",
        "      print(\"Installing dependencies...\")\n",
        "      !pip3 install --disable-pip-version-check --no-cache-dir auraloss==0.4.0 &>> /content/log.txt\n",
        "\n",
        "      print(\"Mounting google drive...\")\n",
        "      from google.colab import drive\n",
        "      drive.mount('/content/drive')\n",
        "    else:\n",
        "      print(\"Code already exists. Skipping Google Drive mounting.\")\n",
        "  else:\n",
        "    print(\"Not running on Google Colab. Skipping Colab-specific setup.\")\n",
        "\n",
        "  # Adjust the env\n",
        "  os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:2\"\n",
        "\n",
        "  from colab_functions import wav2tensor, extract_best_esr_model, create_csv_aidax\n",
        "  from prep_wav_alt import WavParse\n",
        "  import plotly.graph_objects as go\n",
        "  from CoreAudioML.networks import load_model\n",
        "  import CoreAudioML.miscfuncs as miscfuncs\n",
        "  if any(key.startswith(\"COLAB_\") for key in os.environ):\n",
        "    from google.colab import files\n",
        "  import io\n",
        "  import shutil\n",
        "  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "  step = 0\n",
        "  print()\n",
        "  print(\"Ready! you can now move to step 2: DATA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnFHaYnUnlux"
      },
      "source": [
        "# 2. The Data (upload + preprocessing) üìë"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CQleTk7GJV8"
      },
      "source": [
        "### Step 2.1: Download the capture signal\n",
        "Download the pre-crafted \"capture signal\" called [input.wav](https://drive.google.com/file/d/1TNpaPPc9tdCu6OA1VETWvufc7wG2nQTJ/view?usp=sharing) from the provided link.\n",
        "\n",
        "### Step 2.2 Reamp your gear\n",
        "Use the downloaded capture signal to reamp the gear that you want to model. Record the output and save it as \"target.wav\".\n",
        "For a detailed demonstration of how to reamp your gear using the capture signal, refer to this [video tutorial](https://youtu.be/lrvuODtk9W0?t=70) starting at 1:10 and ending at 3:44."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ByCjJDMU25jc"
      },
      "outputs": [],
      "source": [
        "#@markdown `<- RUN CELL (‚ñ∫)`\n",
        "\n",
        "#@markdown Step 2.3 upload\n",
        "#@markdown ---\n",
        "#@markdown * In drive, put the audio files with which you would like to train in a single folder.\n",
        "#@markdown  * `input.wav` : contains the reference (dry/DI) sound.\n",
        "#@markdown  * `target_gainX.X_toneX.X.wav` : contains the target (amped/with effects and selected 2 settins) sound.\n",
        "#@markdown You will need 20 files, named as listed below: \n",
        "#@markdown  * target_gain0.0_tone1.0.wav\n",
        "#@markdown  * target_gain0.1_tone1.0.wav\n",
        "#@markdown  * target_gain0.2_tone1.0.wav\n",
        "#@markdown  * target_gain0.3_tone1.0.wav\n",
        "#@markdown  * target_gain0.4_tone1.0.wav\n",
        "#@markdown  * target_gain0.5_tone0.0.wav\n",
        "#@markdown  * target_gain0.5_tone0.1.wav\n",
        "#@markdown  * target_gain0.5_tone0.2.wav\n",
        "#@markdown  * target_gain0.5_tone0.3.wav\n",
        "#@markdown  * target_gain0.5_tone0.4.wav\n",
        "#@markdown  * target_gain0.5_tone0.5.wav\n",
        "#@markdown  * target_gain0.5_tone0.6.wav\n",
        "#@markdown  * target_gain0.5_tone0.7.wav\n",
        "#@markdown  * target_gain0.5_tone0.8.wav\n",
        "#@markdown  * target_gain0.5_tone0.9.wav\n",
        "#@markdown  * target_gain0.5_tone1.0.wav\n",
        "#@markdown  * target_gain0.6_tone1.0.wav\n",
        "#@markdown  * target_gain0.7_tone1.0.wav\n",
        "#@markdown  * target_gain0.8_tone1.0.wav\n",
        "#@markdown  * target_gain0.9_tone1.0.wav\n",
        "#@markdown  * target_gain1.0_tone1.0.wav\n",
        "#@markdown * Use the file browser in the left panel to find a folder with your audio, right-click **\"Copy Path\", paste below**, and run the cell.\n",
        "#@markdown  * ex. `/content/Aida-x-ConditionedModelsTrainer/Recordings`\n",
        "DATA_DIR = '' #@param {type: \"string\"}\n",
        "\n",
        "assert 'step' in locals(), \"Please run the code in the introduction section first!\"\n",
        "print(\"---\")\n",
        "assert DATA_DIR != '', \"Please input a path for your DATA_DIR\"\n",
        "assert os.path.exists(DATA_DIR), f\"Drive Folder Doesn\\'t Exists: {DATA_DIR}\"\n",
        "# assert set([\"input.wav\", \"target.wav\"]) <= set([x.lower() for x in os.listdir(DATA_DIR)]), \\\n",
        "#  \"Make sure you have \\\"input.wav\\\" and \\\"target_*.wav\\\" inside your data folder\"\n",
        "\n",
        "# Copy the files to /content/ and overwrite if they already exist using bash commands\n",
        "destination_dir = \"/content/Aida-x-ConditionedModelsTrainer\"\n",
        "input_path = os.path.join(destination_dir, \"input.wav\")\n",
        "# target_path = os.path.join(destination_dir, \"target.wav\")\n",
        "targetlevel10edge10path = os.path.join(destination_dir, \"target_level1.0_edge1.0.wav\")\n",
        "targetlevel10edge00path = os.path.join(destination_dir, \"target_level1.0_edge0.0.wav\")\n",
        "targetlevel09edge09path = os.path.join(destination_dir, \"target_level0.9_edge0.9.wav\")\n",
        "targetlevel09edge01path = os.path.join(destination_dir, \"target_level0.9_edge0.1.wav\")\n",
        "targetlevel01edge01path = os.path.join(destination_dir, \"target_level0.1_edge0.1.wav\")\n",
        "targetlevel02edge08path = os.path.join(destination_dir, \"target_level0.2_edge0.8.wav\")\n",
        "targetlevel03edge03path = os.path.join(destination_dir, \"target_level0.3_edge0.3.wav\")\n",
        "targetlevel04edge07path = os.path.join(destination_dir, \"target_level0.4_edge0.7.wav\")\n",
        "targetlevel00edge05path = os.path.join(destination_dir, \"target_level0.0_edge0.5.wav\")\n",
        "targetlevel05edge00path = os.path.join(destination_dir, \"target_level0.5_edge0.0.wav\")\n",
        "targetlevel06edge02path = os.path.join(destination_dir, \"target_level0.6_edge0.2.wav\")\n",
        "targetlevel05edge01path = os.path.join(destination_dir, \"target_level0.5_edge0.1.wav\")\n",
        "targetlevel05edge02path = os.path.join(destination_dir, \"target_level0.5_edge0.2.wav\")\n",
        "targetlevel05edge03path = os.path.join(destination_dir, \"target_level0.5_edge0.3.wav\")\n",
        "targetlevel05edge04path = os.path.join(destination_dir, \"target_level0.5_edge0.4.wav\")\n",
        "targetlevel05edge05path = os.path.join(destination_dir, \"target_level0.5_edge0.5.wav\")\n",
        "targetlevel05edge06path = os.path.join(destination_dir, \"target_level0.5_edge0.6.wav\")\n",
        "targetlevel05edge07path = os.path.join(destination_dir, \"target_level0.5_edge0.7.wav\")\n",
        "targetlevel05edge08path = os.path.join(destination_dir, \"target_level0.5_edge0.8.wav\")\n",
        "targetlevel05edge09path = os.path.join(destination_dir, \"target_level0.5_edge0.9.wav\")\n",
        "targetlevel05edge10path = os.path.join(destination_dir, \"target_level0.5_edge1.0.wav\")\n",
        "\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'input.wav')}\" \"{input_path}\"\n",
        "print(f\"File copied: {input_path}\")\n",
        "\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level1.0_edge1.0.wav')}\" \"{targetlevel10edge10path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level1.0_edge0.0.wav')}\" \"{targetlevel10edge00path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.9_edge0.9.wav')}\" \"{targetlevel09edge09path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.9_edge0.1.wav')}\" \"{targetlevel09edge01path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.1_edge0.1.wav')}\" \"{targetlevel01edge01path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.2_edge0.8.wav')}\" \"{targetlevel02edge08path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.3_edge0.3.wav')}\" \"{targetlevel03edge03path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.4_edge0.7.wav')}\" \"{targetlevel04edge07path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.0_edge0.5.wav')}\" \"{targetlevel00edge05path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.5_edge0.0.wav')}\" \"{targetlevel05edge00path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.6_edge0.2.wav')}\" \"{targetlevel06edge02path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.5_edge0.1.wav')}\" \"{targetlevel05edge01path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.5_edge0.2.wav')}\" \"{targetlevel05edge02path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.5_edge0.3.wav')}\" \"{targetlevel05edge03path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.5_edge0.4.wav')}\" \"{targetlevel05edge04path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.5_edge0.5.wav')}\" \"{targetlevel05edge05path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.5_edge0.6.wav')}\" \"{targetlevel05edge06path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.5_edge0.7.wav')}\" \"{targetlevel05edge07path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.5_edge0.8.wav')}\" \"{targetlevel05edge08path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.5_edge0.9.wav')}\" \"{targetlevel05edge09path}\"\n",
        "!cp -f \"{os.path.join(DATA_DIR, 'target_level0.5_edge1.0.wav')}\" \"{targetlevel05edge10path}\"\n",
        "\n",
        "print(f\"Files copied.\")\n",
        "\n",
        "#@markdown Choose the Model type you want to train:\\\n",
        "#@markdown Generally, the heavier the model the more accurate it is, but also the more CPU it consumes.\n",
        "#@markdown Here's a list of approximate CPU consumption of each model type on a [MOD Dwarf](https://mod.audio/dwarf/):\n",
        "#@markdown * Lightest: 25% CPU\n",
        "#@markdown * Light: 30% CPU\n",
        "#@markdown * Standard: 37% CPU\n",
        "#@markdown * Heavy: 46% CPU\n",
        "model_type = \"Standard\" #@param [\"Lightest\", \"Light\", \"Standard\", \"Heavy\", \"BadCat\"]\n",
        "\n",
        "if model_type == \"Lightest\":\n",
        "  config_file = \"LSTM-8-1\"\n",
        "elif model_type == \"Light\":\n",
        "  config_file = \"LSTM-12-1\"\n",
        "elif model_type == \"Standard\":\n",
        "  config_file = \"LSTM-16-1\"\n",
        "elif model_type == \"Heavy\":\n",
        "  config_file = \"LSTM-20-1\"\n",
        "elif model_type == \"BadCat\":\n",
        "  config_file = \"Bad_Cat_CH1\"\n",
        "\n",
        "# Create the CSV and parse the WAV files\n",
        "create_csv_aidax(\"/content/Aida-x-ConditionedModelsTrainer/Configs/Csv/modaudioug.csv\")\n",
        "WavParse(load_config=config_file, config_location='/content/Aida-x-ConditionedModelsTrainer/Configs', norm=False, denoise=False)\n",
        "\n",
        "step = max(step, 1)\n",
        "print()\n",
        "print(\"Data prepared! You can now move to step 3: TRAINING\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NMA3O0FnsKP"
      },
      "source": [
        "# 3. Model Training üèãÔ∏è‚Äç‚ôÇÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h6RdceOeWdZl"
      },
      "outputs": [],
      "source": [
        "#@markdown `<- RUN CELL (‚ñ∫)`\n",
        "\n",
        "#@markdown Training usually takes around 10 minutes,\n",
        "#@markdown but this can change depending on the duration of\n",
        "#@markdown the training data that you provided and the model_type\n",
        "#@markdown you choose.\\\n",
        "#@markdown Note that training doesn't always lead to the same results.\n",
        "#@markdown You may want to run it a couple of times and compare the results.\n",
        "\n",
        "# #@markdown Choose the Model type you want to train:\\\n",
        "# #@markdown Generally, the heavier the model the more accurate it is, but also the more CPU it consumes.\n",
        "# #@markdown Here's a list of approximate CPU consumption of each model type on a [MOD Dwarf](https://mod.audio/dwarf/):\n",
        "# #@markdown * Lightest: 25% CPU\n",
        "# #@markdown * Light: 30% CPU\n",
        "# #@markdown * Standard: 37% CPU\n",
        "# #@markdown * Heavy: 46% CPU\n",
        "# model_type = \"Standard\" #@param [\"Lightest\", \"Light\", \"Standard\", \"Heavy\"]\n",
        "#@markdown Some training hyper parameters\n",
        "#@markdown (Recommended: ignore and continue with default values):\n",
        "skip_connection = \"OFF\" #@param [\"ON\", \"OFF\"]\n",
        "epochs = 200 #@param {type:\"slider\", min:100, max:2000, step:20}\n",
        "print(\"---\")\n",
        "\n",
        "# if model_type == \"Lightest\":\n",
        "#   config_file = \"LSTM-8-1\"\n",
        "# elif model_type == \"Light\":\n",
        "#   config_file = \"LSTM-12-1\"\n",
        "# elif model_type == \"Standard\":\n",
        "#   config_file = \"LSTM-16-1\"\n",
        "# elif model_type == \"Heavy\":\n",
        "#   config_file = \"LSTM-20-1\"\n",
        "\n",
        "if skip_connection == \"ON\":\n",
        "  skip_con = 1\n",
        "else:\n",
        "  skip_con = 0\n",
        "\n",
        "assert 'step' in locals(), \"Please run the code in the introduction section first!\"\n",
        "assert step>=1, \"Please execute the \\\"1.DATA\\\" cell code to prepare the data for the training!\"\n",
        "\n",
        "!python3 dist_model.py -l \"$config_file\" -lm 0 -sc $skip_con -eps $epochs\n",
        "\n",
        "sleep(1)\n",
        "model_dir = f\"/content/Aida-x-ConditionedModelsTrainer/Results/MOD-AUDIO-UG\"\n",
        "step = max(step, 2)\n",
        "print(\"Training done!\\nESR after training: \", extract_best_esr_model(model_dir)[1])\n",
        "print(\"You can now move to step 4: EVALUATION or directly to step 5: EXPORT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxglIXwBn1sj"
      },
      "source": [
        "# 4. Model Evaluation üìà\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QalvHphTImo_"
      },
      "outputs": [],
      "source": [
        "#@markdown `<- RUN CELL (‚ñ∫)`\n",
        "\n",
        "#@markdown Here you can visualize and listen to the output of your trained model on the data you provided earlier.\n",
        "\n",
        "assert 'step' in locals(), \"Please run the code in the introduction section first!\"\n",
        "assert step>=1, \"Please execute the \\\"1.DATA\\\" cell code to prepare the data for the training!\"\n",
        "assert step>=2, \"Please execute the \\\"2.TRAINING\\\" cell code to train a model for evaluation!\"\n",
        "\n",
        "print(\"---\")\n",
        "# Find the file with .full_name extension in model_dir\n",
        "full_name_file = [f for f in os.listdir(model_dir) if f.endswith('.full_name')]\n",
        "assert len(full_name_file) == 1, \"There should be exactly one file with the .full_name extension in the model_dir.\"\n",
        "\n",
        "# Remove the .full_name extension to create the model_filename\n",
        "model_filename = os.path.splitext(full_name_file[0])[0] + '.aidax'\n",
        "\n",
        "# Extract the best model available from training results\n",
        "model_path, esr = extract_best_esr_model(model_dir)\n",
        "model_data = miscfuncs.json_load(model_path)\n",
        "model = load_model(model_data).to(device)\n",
        "\n",
        "full_dry = wav2tensor(f\"/content/Aida-x-ConditionedModelsTrainer/Data/test/aidadsp-auto-input.wav\")\n",
        "full_amped = wav2tensor(f\"/content/Aida-x-ConditionedModelsTrainer/Data/test/aidadsp-auto-target.wav\")\n",
        "\n",
        "samples_viz = 24000\n",
        "duration_audio = 5\n",
        "seg_length = int(duration_audio * 48000)\n",
        "start_sample = np.random.randint(len(full_dry)-duration_audio*48000)\n",
        "dry = full_dry[start_sample:start_sample+seg_length]\n",
        "amped = full_amped[start_sample:start_sample+seg_length]\n",
        "with torch.no_grad():\n",
        "  modeled = model(dry[:, None, None].to(device)).cpu().flatten().detach().numpy()\n",
        "\n",
        "print(f\"Current model: {model_filename}\")\n",
        "print(f\"ESR:\", esr)\n",
        "# Visualization\n",
        "fig = go.Figure()\n",
        "fig.add_trace(\n",
        "  go.Scatter(\n",
        "    x=list(np.arange(len(dry[:samples_viz]))/48000), y=dry[:samples_viz],\n",
        "    name=\"dry\", mode='lines'\n",
        "  )\n",
        ")\n",
        "fig.add_trace(\n",
        "  go.Scatter(\n",
        "    x=list(np.arange(len(amped[:samples_viz]))/48000), y=amped[:samples_viz],\n",
        "    name=\"target\", mode='lines'\n",
        "  )\n",
        ")\n",
        "fig.add_trace(\n",
        "  go.Scatter(\n",
        "    x=list(np.arange(len(modeled[:samples_viz]))/48000), y=modeled[:samples_viz],\n",
        "    name=\"prediction\", mode='lines'\n",
        "  )\n",
        ")\n",
        "fig.update_layout(\n",
        "  title=\"Dry vs Target vs Predicted signal\",\n",
        "  xaxis_title=\"Time (s)\",\n",
        "  yaxis_title=\"Signal Amplitude\",\n",
        "  legend_title=\"Signal\",\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# Listen\n",
        "print(\"DRY Signal:\")\n",
        "IPython.display.display(IPython.display.Audio(data=dry, rate=48000))\n",
        "\n",
        "print(\"TARGET Signal:\")\n",
        "IPython.display.display(IPython.display.Audio(data=amped, rate=48000))\n",
        "\n",
        "print(\"PREDICTED Signal:\")\n",
        "IPython.display.display(IPython.display.Audio(data=modeled, rate=48000))\n",
        "\n",
        "print(\"Difference Signal:\")\n",
        "difference_signal = np.array(amped) - np.array(modeled)\n",
        "IPython.display.display(IPython.display.Audio(data=difference_signal, rate=48000))\n",
        "\n",
        "# Cleanup\n",
        "del dry, amped, modeled, full_dry, full_amped, model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "step = max(step, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B-9xp6fxmLmI"
      },
      "outputs": [],
      "source": [
        "#@markdown `<- RUN CELL (‚ñ∫)`\n",
        "\n",
        "#@markdown Here you can **upload** your own dry guitar files, and listen to the predicted output of the model.\n",
        "assert 'step' in locals(), \"Please run the code in the introduction section first!\"\n",
        "assert step>=1, \"Please execute the \\\"1.DATA\\\" cell code to prepare the data for the training!\"\n",
        "assert step>=2, \"Please execute the \\\"2.TRAINING\\\" cell code to train a model for evaluation!\"\n",
        "\n",
        "print(\"---\")\n",
        "\n",
        "if any(key.startswith(\"COLAB_\") for key in os.environ):\n",
        "  uploaded = files.upload()\n",
        "print()\n",
        "print(\"Running predictions:\")\n",
        "\n",
        "for k, v in uploaded.items():\n",
        "  print(\"#####\", k)\n",
        "  dry = wav2tensor(io.BytesIO(v))\n",
        "  with torch.no_grad():\n",
        "    modeled = model(dry[:, None, None].to(device)).cpu().flatten().detach().numpy()\n",
        "\n",
        "  print(\"DRY Signal:\")\n",
        "  IPython.display.display(IPython.display.Audio(data=dry, rate=48000))\n",
        "\n",
        "  print(\"PREDICTED Signal:\")\n",
        "  IPython.display.display(IPython.display.Audio(data=modeled, rate=48000))\n",
        "\n",
        "  # Cleanup\n",
        "  del dry, modeled\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "step = max(step, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjYqtq7FoK0Z"
      },
      "source": [
        "# 5. Model Export ‚úÖ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aTXO1011OWVI"
      },
      "outputs": [],
      "source": [
        "#@markdown `<- RUN CELL (‚ñ∫)`\n",
        "\n",
        "#@markdown Download a .aidax file summarizing the model that you just trained.\n",
        "\n",
        "#@markdown You can then upload it to AIDA-X model loader plugin and run it in real-time.\n",
        "\n",
        "assert 'step' in locals(), \"Please run the code in the introduction section first!\"\n",
        "assert step>=1, \"Please execute the \\\"1.DATA\\\" cell code to prepare the data for the training!\"\n",
        "assert step>=2, \"Please execute the \\\"2.TRAINING\\\" cell code to train a model for evaluation!\"\n",
        "\n",
        "print(\"---\")\n",
        "# Find the file with .full_name extension in model_dir\n",
        "full_name_file = [f for f in os.listdir(model_dir) if f.endswith('.full_name')]\n",
        "assert len(full_name_file) == 1, \"There should be exactly one file with the .full_name extension in the model_dir.\"\n",
        "\n",
        "# Remove the .full_name extension to create the model_filename\n",
        "model_filename = os.path.splitext(full_name_file[0])[0] + '.aidax'\n",
        "\n",
        "print(\"Generating model file:\", model_filename)\n",
        "\n",
        "# Extract the best model available from training results\n",
        "model_path, esr = extract_best_esr_model(model_dir)\n",
        "!python3 modelToRTNeural.py -l \"$config_file\" -ax\n",
        "\n",
        "# Define the destination directory\n",
        "destination_path = os.path.join(DATA_DIR, model_filename)\n",
        "\n",
        "# Copy the generated file to the destination directory\n",
        "!cp \"{os.path.join(model_dir, 'model_rtneural.aidax')}\" \"{destination_path}\"\n",
        "\n",
        "if any(key.startswith(\"COLAB_\") for key in os.environ):\n",
        "  from google.colab import files\n",
        "  files.download(destination_path)\n",
        "\n",
        "print()\n",
        "print(\"Model file saved to:\", destination_path)\n",
        "step = max(step, 4)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
